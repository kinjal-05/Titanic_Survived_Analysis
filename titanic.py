# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11aCY8-kpGDCOUZKERnvDobZiSWcbmtD-
"""

from google.colab import files
import pandas as pd
import json

# Upload file
uploaded = files.upload()

# Get filename
filename = list(uploaded.keys())[0]
print("Uploaded file:", filename)

df = pd.read_json(filename)
print(df.head())

# Number of rows in JSON file after loading into DataFrame
num_rows = df.shape[0]
print(f"The dataset has {num_rows} rows.")

# List of all columns in the DataFrame
columns_list = df.columns.tolist()
print("Columns in the dataset:")
print(columns_list)

# Summary statistics
print("Summary statistics for 'Survived':")
print(df['Survived'].describe())

# Count of each category (0 = did not survive, 1 = survived)
print("\nCounts of each class:")
print(df['Survived'].value_counts())

# Percentage distribution
print("\nPercentage of each class:")
print(df['Survived'].value_counts(normalize=True) * 100)

# Check missing values
print(f"\nNumber of missing values: {df['Survived'].isnull().sum()}")

import seaborn as sns
import matplotlib.pyplot as plt

# Bar plot for Survived column
sns.countplot(x='Survived', data=df, palette='pastel')
plt.title("Survival Count")
plt.xlabel("Survived (0 = No, 1 = Yes)")
plt.ylabel("Number of Passengers")
plt.show()

# Summary statistics
print("Summary statistics for 'Pclass':")
print(df['Pclass'].describe())

# Count of each class
print("\nCounts of each class:")
print(df['Pclass'].value_counts())

# Percentage distribution
print("\nPercentage of each class:")
print(df['Pclass'].value_counts(normalize=True) * 100)

# Check for missing values
print(f"\nNumber of missing values: {df['Pclass'].isnull().sum()}")

import seaborn as sns
import matplotlib.pyplot as plt

# Bar plot for Pclass column
sns.countplot(x='Pclass', data=df, palette='pastel')
plt.title("Passenger Class Distribution")
plt.xlabel("Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)")
plt.ylabel("Number of Passengers")
plt.show()

# Summary statistics for 'Name'
print("Summary statistics for 'Name':")
print(df['Name'].describe())

# Check missing values
print(f"\nNumber of missing values in 'Name': {df['Name'].isnull().sum()}")

# Show a few sample names
print("\nFirst 5 passenger names:")
print(df['Name'].head())

# Remove 'Name' column
if 'Name' in df.columns:
    df = df.drop(columns=['Name'])

print("Columns after removing 'Name':")
print(df.columns.tolist())

# Summary statistics for 'Sex'
print("Summary statistics for 'Sex':")
print(df['Sex'].describe())

# Count of each category
print("\nCounts of each sex:")
print(df['Sex'].value_counts())

# Percentage distribution
print("\nPercentage of each sex:")
print(df['Sex'].value_counts(normalize=True) * 100)

# Check for missing values
print(f"\nNumber of missing values in 'Sex': {df['Sex'].isnull().sum()}")

import numpy as np

# Replace invalid values in 'Sex' with NaN
df.loc[~df['Sex'].isin(['male', 'female']), 'Sex'] = np.nan

print("Unique values in 'Sex' after cleaning:")
print(df['Sex'].unique())

# Replace NaN in 'Sex' with the value from 'Age' column
df.loc[df['Sex'].isnull(), 'Sex'] = df['Age']

# Verify the changes
print("Sample rows where Sex was NaN and replaced with Age:")
print(df[df['Sex'] == df['Age']].head(10))

import seaborn as sns
import matplotlib.pyplot as plt

# Countplot including NaN (fill NaN with label first)
sns.countplot(x=df['Sex'].fillna('Missing'), palette='pastel')

plt.title("Passenger Gender Distribution (with Missing Values)")
plt.xlabel("Sex")
plt.ylabel("Number of Passengers")
plt.show()

# Summary statistics for 'Age'
print("Summary statistics for 'Age':")
print(df['Age'].describe())

# Number of missing values
missing_age = df['Age'].isnull().sum()
print(f"\nNumber of missing values in 'Age': {missing_age}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(df['Age'], bins=30, kde=True, color='skyblue')

plt.title("Age Distribution of Passengers")
plt.xlabel("Age")
plt.ylabel("Number of Passengers")
plt.show()

# Check missing values in Age column
missing_age = df['Age'].isnull().sum()

if missing_age > 0:
    print(f"'Age' column has {missing_age} missing values.")
else:
    print("'Age' column has no missing values.")

# Unique values in Age column (excluding NaN)
unique_ages = df['Age'].dropna().unique()

print(f"Number of unique ages: {len(unique_ages)}")
print("Sample of unique ages:", sorted(unique_ages))  # first 20 sorted ages

import numpy as np

# Replace empty strings with NaN
df['Age'] = df['Age'].replace('', np.nan)

# Verify
print("Sample values in 'Age' after replacement:")
print(df['Age'].head(10))
print(f"\nNumber of missing values now: {df['Age'].isnull().sum()}")

# Unique values in Age column (excluding NaN)
unique_ages = df['Age'].dropna().unique()

print(f"Number of unique ages: {len(unique_ages)}")
print("Sample of unique ages:", sorted(unique_ages))  # first 20 sorted ages

# Replace NaN in 'Age' with the value from 'SibSp' column
df.loc[df['Age'].isnull(), 'Age'] = df['SibSp']

# Verify the changes
print("Sample rows where Age was NaN and replaced with SibSp:")
print(df[df['Age'] == df['SibSp']].head(10))

import numpy as np
import pandas as pd

# Step 1: Replace empty strings with NaN
df['Age'] = df['Age'].replace('', np.nan)

# Step 2: Convert column to numeric (floats), coerce errors to NaN
df['Age'] = pd.to_numeric(df['Age'], errors='coerce')

# Step 3: Round non-NaN values to nearest integer
df['Age'] = df['Age'].apply(lambda x: int(round(x)) if pd.notnull(x) else x)

# Verify
print("Unique values in 'Age' column after conversion:")
print(df['Age'].unique())

print("\nData type of 'Age' column:")
print(df['Age'].dtype)

df['Age'] = df['Age'].astype('Int64')

# Verify
print("Unique values in 'Age' column after conversion:")
print(df['Age'].unique())

print("\nData type of 'Age' column:")
print(df['Age'].dtype)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))

# Histogram with density curve
sns.histplot(df['Age'], bins=30, kde=True, color='skyblue')

plt.title("Age Distribution of Passengers")
plt.xlabel("Age")
plt.ylabel("Number of Passengers")
plt.show()

# Check missing values in Age column
missing_age = df['Age'].isnull().sum()

if missing_age > 0:
    print(f"'Age' column has {missing_age} missing values.")
else:
    print("'Age' column has no missing values.")

# Summary statistics for 'SibSp'
print("Summary statistics for 'SibSp':")
print(df['SibSp'].describe())

# Value counts
print("\nCounts of each value in 'SibSp':")
print(df['SibSp'].value_counts().sort_index())

# Percentage distribution
print("\nPercentage distribution of 'SibSp':")
print(df['SibSp'].value_counts(normalize=True).sort_index() * 100)

# Missing values
print(f"\nNumber of missing values in 'SibSp': {df['SibSp'].isnull().sum()}")

import seaborn as sns
import matplotlib.pyplot as plt

# Bar plot for SibSp
sns.countplot(x='SibSp', data=df, palette='pastel')
plt.title("Distribution of Siblings/Spouses Aboard (SibSp)")
plt.xlabel("Number of Siblings/Spouses")
plt.ylabel("Number of Passengers")
plt.show()

# Unique values in SibSp column
unique_sibsp = df['SibSp'].dropna().unique()  # drop NaN if any

print("Different types of values in 'SibSp' column:")
print(sorted(unique_sibsp))
print(f"\nTotal unique values: {len(unique_sibsp)}")

import numpy as np

# Replace empty strings with NaN
df['SibSp'] = df['SibSp'].replace('', np.nan)

# Verify
print("Sample values in 'SibSp' after replacement:")
print(df['SibSp'].head(10))
print(f"\nNumber of missing values now: {df['SibSp'].isnull().sum()}")

# Unique values in SibSp column
unique_sibsp = df['SibSp'].unique()  # drop NaN if any

print("Different types of values in 'SibSp' column:")
print((unique_sibsp))

import numpy as np

# Replace empty strings with NaN
df['SibSp'] = df['SibSp'].replace('', np.nan)

# Get unique values excluding NaN
unique_sibsp = df['SibSp'].dropna().unique()

print("Different types of values in 'SibSp' column ('' replaced with NaN):")
print(sorted(unique_sibsp))
print(f"\nTotal unique values: {len(unique_sibsp)}")

# Replace NaN in 'SibSp' with the value from 'Parch' column
df.loc[df['SibSp'].isnull(), 'SibSp'] = df['Parch']

# Verify the changes
print("Sample rows where SibSp was NaN and replaced with Parch:")
print(df[df['SibSp'] == df['Parch']].head(10))

import numpy as np
import pandas as pd

# Convert column to numeric, coerce errors (like non-numeric strings) to NaN
df['SibSp'] = pd.to_numeric(df['SibSp'], errors='coerce')

# Round non-NaN values to nearest integer
df['SibSp'] = df['SibSp'].apply(lambda x: int(round(x)) if pd.notnull(x) else x)

# Verify the changes
print(df['SibSp'].unique())
print(df['SibSp'].dtypes)

# Convert SibSp to numeric first (if not already)
df['SibSp'] = pd.to_numeric(df['SibSp'], errors='coerce')

# Convert to nullable integer type (keeps NaN as is)
df['SibSp'] = df['SibSp'].astype('Int64')

# Verify
print(df['SibSp'].unique())
print(df['SibSp'].dtypes)

import seaborn as sns
import matplotlib.pyplot as plt

# Bar plot for SibSp
sns.countplot(x='SibSp', data=df, palette='pastel')
plt.title("Distribution of Siblings/Spouses Aboard (SibSp)")
plt.xlabel("Number of Siblings/Spouses")
plt.ylabel("Number of Passengers")
plt.show()

# Summary statistics for 'Parch'
print("Summary statistics for 'Parch':")
print(df['Parch'].describe())

# Count of each value
print("\nCounts of each value in 'Parch':")
print(df['Parch'].value_counts().sort_index())

# Percentage distribution
print("\nPercentage distribution of 'Parch':")
print(df['Parch'].value_counts(normalize=True).sort_index() * 100)

# Missing values
print(f"\nNumber of missing values in 'Parch': {df['Parch'].isnull().sum()}")

# Unique values in Parch column (excluding NaN)
unique_parch = df['Parch'].unique()

print("Different types of values in 'Parch' column:")
print(sorted(unique_parch))
print(f"\nTotal unique values: {len(unique_parch)}")

import seaborn as sns
import matplotlib.pyplot as plt

# Bar plot for Parch
sns.countplot(x='Parch', data=df, palette='pastel')
plt.title("Distribution of Parents/Children Aboard (Parch)")
plt.xlabel("Number of Parents/Children")
plt.ylabel("Number of Passengers")
plt.show()

# Summary statistics for 'Ticket'
print("Summary statistics for 'Ticket':")
print(df['Ticket'].describe())

# Number of missing values
print(f"\nNumber of missing values in 'Ticket': {df['Ticket'].isnull().sum()}")

# Different types of unique values in 'Ticket'
unique_tickets = df['Ticket'].dropna().unique()
print(f"\nTotal unique tickets: {len(unique_tickets)}")
print("Sample of unique tickets:", unique_tickets)  # show first 20

# Check if any rows have empty string in 'Ticket' column
empty_rows = df[df['Ticket'] == '']

print(f"Number of rows with empty string in 'Ticket': {len(empty_rows)}")
print("\nSample rows with empty Ticket values:")
print(empty_rows.head())

# Unique values in the column with name ''
unique_values = df[''].dropna().unique()  # drop NaN if any

print("Unique values in the column '':")
print(unique_values)
print(f"\nTotal unique values: {len(unique_values)}")

import numpy as np

# Replace empty strings with NaN in column named ''
df[''] = df[''].replace('', np.nan)

# Verify
print("Sample values in column '' after replacement:")
print(df[''].head(10))
print(f"\nNumber of missing values now: {df[''].isnull().sum()}")

# Replace Ticket with Fare where column '' is NOT NaN
df.loc[df[''].notnull(), 'Ticket'] = df.loc[df[''].notnull(), 'Fare']

# Verify
print("Sample rows where Ticket was replaced with Fare due to '' column NOT being NaN:")
print(df[df[''].notnull()][['Ticket', 'Fare', '']].head(10))

# Summary statistics for 'Ticket'
print("Summary statistics for 'Ticket':")
print(df['Ticket'].describe())

# Number of missing values
print(f"\nNumber of missing values in 'Ticket': {df['Ticket'].isnull().sum()}")

# Different types of unique values in 'Ticket'
unique_tickets = df['Ticket'].dropna().unique()
print(f"\nTotal unique tickets: {len(unique_tickets)}")
print("Sample of unique tickets:", unique_tickets)  # show first 20

# Summary statistics for 'Fare'
print("Summary statistics for 'Fare':")
print(df['Fare'].describe())

# Number of missing values
print(f"\nNumber of missing values in 'Fare': {df['Fare'].isnull().sum()}")

# Unique values in 'Fare' column
unique_fares = df['Fare'].unique()
print(f"\nTotal unique fare values: {len(unique_fares)}")
print("Sample of unique fares:", sorted(unique_fares))  # first 20 sorted fares

# Replace Fare with Cabin where column '' is NOT NaN
df.loc[df[''].notnull(), 'Fare'] = df.loc[df[''].notnull(), 'Cabin']

# Verify
print("Sample rows where Fare was replaced with Cabin due to '' column NOT being NaN:")
print(df[df[''].notnull()][['Fare', 'Cabin', '']].head(10))

# Summary statistics for 'Ticket'
print("Summary statistics for 'Ticket':")
print(df['Ticket'].describe())

# Number of missing values
print(f"\nNumber of missing values in 'Ticket': {df['Ticket'].isnull().sum()}")

# Different types of unique values in 'Ticket'
unique_tickets = df['Ticket'].dropna().unique()
print(f"\nTotal unique tickets: {len(unique_tickets)}")
print("Sample of unique tickets:", unique_tickets)  # show first 20

# Summary statistics for 'Cabin'
print("Summary statistics for 'Cabin':")
print(df['Cabin'].describe())

# Number of missing values
missing_cabin = df['Cabin'].isnull().sum()
print(f"\nNumber of missing values in 'Cabin': {missing_cabin}")

# Unique cabin values
unique_cabins = df['Cabin'].unique()
print(f"\nTotal unique cabin values: {len(unique_cabins)}")
print("Sample of unique cabins:", unique_cabins)  # show first 20

import numpy as np

# Replace empty strings with NaN in Cabin column
df['Cabin'] = df['Cabin'].replace('', np.nan)

# Verify
print("Sample values in 'Cabin' after replacement:")
print(df['Cabin'].head(10))
print(f"\nNumber of missing values now: {df['Cabin'].isnull().sum()}")

# Replace Cabin with Embarked where column '' is NOT NaN
df.loc[df[''].notnull(), 'Cabin'] = df.loc[df[''].notnull(), 'Embarked']

# Verify
print("Sample rows where Cabin was replaced with Embarked due to '' column NOT being NaN:")
print(df[df[''].notnull()][['Cabin', 'Embarked', '']].head(10))

# Summary statistics for 'Cabin'
print("Summary statistics for 'Cabin':")
print(df['Cabin'].describe())

# Number of missing values
missing_cabin = df['Cabin'].isnull().sum()
print(f"\nNumber of missing values in 'Cabin': {missing_cabin}")

# Unique cabin values
unique_cabins = df['Cabin'].unique()
print(f"\nTotal unique cabin values: {len(unique_cabins)}")
print("Sample of unique cabins:", unique_cabins)  # show first 20

# Summary statistics for 'Embarked'
print("Summary statistics for 'Embarked':")
print(df['Embarked'].describe())

# Number of missing values
missing_embarked = df['Embarked'].isnull().sum()
print(f"\nNumber of missing values in 'Embarked': {missing_embarked}")

# Unique values
unique_embarked = df['Embarked'].dropna().unique()
print(f"\nDifferent types of values in 'Embarked': {unique_embarked}")

# Count of each category
print("\nCounts of each Embarked value:")
print(df['Embarked'].value_counts())

# Percentage distribution
print("\nPercentage distribution of each Embarked value:")
print(df['Embarked'].value_counts(normalize=True) * 100)

import numpy as np

# Replace invalid Embarked values with NaN
df.loc[~df['Embarked'].isin(['S', 'C', 'Q']), 'Embarked'] = np.nan

# Verify
print("Unique values in Embarked after cleaning:")
print(df['Embarked'].unique())

print(f"\nNumber of missing values now: {df['Embarked'].isnull().sum()}")

# Replace NaN in Embarked with the value from '' column
df.loc[df['Embarked'].isnull(), 'Embarked'] = df.loc[df['Embarked'].isnull(), '']

# Verify
print("Sample rows where Embarked was NaN and replaced with '' column value:")
print(df[df['Embarked'] == df['']][['Embarked', '']].head(10))

import numpy as np

# Replace invalid Embarked values with NaN
df.loc[~df['Embarked'].isin(['S', 'C', 'Q']), 'Embarked'] = np.nan

# Verify
print("Unique values in Embarked after cleaning:")
print(df['Embarked'].unique())

print(f"\nNumber of missing values now: {df['Embarked'].isnull().sum()}")

# Drop the column named ''
df = df.drop(columns=[''])

# Verify
print("Columns after dropping the '' column:")
print(df.columns)

# Count of null values in each column
null_counts = df.isnull().sum()

print("Number of null/NaN values in each column:")
print(null_counts)

# Drop the 'Cabin' column
df = df.drop(columns=['Cabin'])

# Verify
print("Columns after dropping 'Cabin':")
print(df.columns)

# Count of columns that contain at least one empty string
columns_with_empty_string = df.columns[(df == '').any()]
num_columns_with_empty_string = len(columns_with_empty_string)

print(f"Columns that contain at least one empty string: {list(columns_with_empty_string)}")
print(f"Number of columns with empty string values: {num_columns_with_empty_string}")

# Select columns with numeric dtype
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

print("Columns that have only int or float values:")
print(list(numeric_columns))

# Drop rows where 'Embarked' is NaN
df = df.dropna(subset=['Embarked'])

# Verify
print(f"Number of rows after removing NaN in 'Embarked': {len(df)}")

# Drop the 'Ticket' column
df = df.drop(columns=['Ticket'])

# Verify
print("Columns after dropping 'Ticket':")
print(df.columns)

import pandas as pd
import numpy as np

# Convert Fare to numeric, coercing errors (non-numeric values) to NaN
df['Fare'] = pd.to_numeric(df['Fare'], errors='coerce')

# Verify
print("Sample values in Fare after converting non-numeric to NaN:")
print(df['Fare'].head(10))

# Count of NaN values after conversion
print(f"\nNumber of NaN values in Fare: {df['Fare'].isnull().sum()}")

# Get unique values in Fare column
unique_fares = df['Fare'].unique()  # drop NaN if any

print("Unique values in Fare column:")
print(unique_fares)

print(f"\nTotal unique values: {len(unique_fares)}")

from sklearn.preprocessing import OneHotEncoder

# Initialize OneHotEncoder
# sparse=False is deprecated and removed in newer versions of scikit-learn
# By default, OneHotEncoder returns a sparse matrix. Use .toarray() if a dense array is needed.
ohe = OneHotEncoder(drop=None)

# Fit and transform the Embarked column
embarked_encoded = ohe.fit_transform(df[['Embarked']])

# Create a DataFrame with the encoded values and proper column names
# Convert to a dense array if needed for further operations
embarked_encoded_df = pd.DataFrame(embarked_encoded.toarray().astype(int), columns=ohe.get_feature_names_out(['Embarked']))

# Concatenate with original DataFrame
# Reset index before concatenating to avoid alignment issues
df = df.reset_index(drop=True)
df = pd.concat([df, embarked_encoded_df], axis=1)


# Optionally, drop the original Embarked column
# df = df.drop(columns=['Embarked'])

# Verify
print("Sample of DataFrame after OneHotEncoder on Embarked:")
print(df.head(10))

from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Initialize OneHotEncoder
# sparse=False is deprecated and removed in newer versions of scikit-learn
# By default, OneHotEncoder returns a sparse matrix. Use .toarray() if a dense array is needed.
ohe = OneHotEncoder(drop=None)

# Fit and transform the Sex column
sex_encoded = ohe.fit_transform(df[['Sex']])

# Convert to DataFrame and cast to int
sex_encoded_df = pd.DataFrame(sex_encoded.toarray(),
                              columns=ohe.get_feature_names_out(['Sex'])).astype(int)

# Concatenate with original DataFrame
df = pd.concat([df, sex_encoded_df], axis=1)

# Optionally, drop the original 'Sex' column
# df = df.drop(columns=['Sex'])

# Verify
print("One-hot encoded Sex column as integers:")
print(df.head(10))

import pandas as pd

# Assuming your DataFrame is called df
df = df.drop(['Sex', 'Embarked'], axis=1)

# To check the result
print(df.head())

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming your DataFrame is df
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

# Remove 'Age' from numeric_features to avoid plotting Age vs Age
numeric_features.remove('Age')

# Scatter plots for numeric features vs Age
for col in numeric_features:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=col, y='Age', data=df)
    plt.title(f'Age vs {col}')
    plt.show()

# Boxplots for categorical features vs Age
for col in categorical_features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=col, y='Age', data=df)
    plt.title(f'Age vs {col}')
    plt.show()

# Check columns with null values
null_columns = df.isnull().sum()
print(null_columns[null_columns > 0])

import seaborn as sns
import matplotlib.pyplot as plt

# Separate numeric and categorical features
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

# Remove 'Survived' from numeric_features if present
if 'Survived' in numeric_features:
    numeric_features.remove('Survived')

# Plot Survived vs numeric features (boxplots)
for col in numeric_features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x='Survived', y=col, data=df)
    plt.title(f'Survived vs {col}')
    plt.show()

# Plot Survived vs categorical features (countplots)
for col in categorical_features:
    plt.figure(figsize=(6, 4))
    sns.countplot(x=col, hue='Survived', data=df)
    plt.title(f'Survived vs {col}')
    plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Numeric features vs Survived → line plot of mean values
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if 'Survived' in numeric_features:
    numeric_features.remove('Survived')

for col in numeric_features:
    plt.figure(figsize=(6,4))
    mean_values = df.groupby('Survived')[col].mean().reset_index()
    sns.lineplot(x='Survived', y=col, data=mean_values, marker='o')
    plt.title(f'Mean {col} by Survived')
    plt.show()

# Categorical features vs Survived → bar plot of proportions
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

for col in categorical_features:
    plt.figure(figsize=(6,4))
    prop = df.groupby([col, 'Survived']).size().reset_index(name='count')
    sns.barplot(x=col, y='count', hue='Survived', data=prop)
    plt.title(f'{col} vs Survived')
    plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Numeric features vs Survived → scatter (jittered for Survived)
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if 'Survived' in numeric_features:
    numeric_features.remove('Survived')

for col in numeric_features:
    plt.figure(figsize=(6,4))
    sns.stripplot(x='Survived', y=col, data=df, jitter=True, size=6, alpha=0.7)
    plt.title(f'{col} vs Survived (Dot Plot)')
    plt.show()

# Optional: Categorical features → scatter-like using swarmplot
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

for col in categorical_features:
    plt.figure(figsize=(6,4))
    sns.swarmplot(x=col, y='Survived', data=df, size=6)
    plt.title(f'{col} vs Survived (Dot Plot)')
    plt.show()

# Calculate mean age as integer
mean_age = int(df['Age'].mean())

# Replace NaN values in Age column with mean_age
df['Age'].fillna(mean_age, inplace=True)

# Check if any NaN values remain in Age
print(df['Age'].isnull().sum())

# Count of null values in each column
null_columns = df.isnull().sum()

# Show only columns that have at least one null value
columns_with_null = null_columns[null_columns > 0]
print(columns_with_null)

# Save DataFrame to CSV
df.to_csv('kinjal.csv', index=False)

print("DataFrame saved to kinjal.csv")

import seaborn as sns
import matplotlib.pyplot as plt

# Numeric features vs Survived → scatter (jittered for Survived)
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if 'Survived' in numeric_features:
    numeric_features.remove('Survived')

for col in numeric_features:
    plt.figure(figsize=(6,4))
    sns.stripplot(x='Survived', y=col, data=df, jitter=True, size=6, alpha=0.7)
    plt.title(f'{col} vs Survived (Dot Plot)')
    plt.show()

# Optional: Categorical features → scatter-like using swarmplot
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

for col in categorical_features:
    plt.figure(figsize=(6,4))
    sns.swarmplot(x=col, y='Survived', data=df, size=6)
    plt.title(f'{col} vs Survived (Dot Plot)')
    plt.show()

# List of all columns
columns_list = df.columns.tolist()
print(columns_list)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import tree
import matplotlib.pyplot as plt

# Define input features and target
X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
        'Embarked_C', 'Embarked_Q', 'Embarked_S',
        'Sex_female', 'Sex_male']]
y = df['Survived']

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Decision Tree model
dt_model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)

# Train the model
dt_model.fit(X_train, y_train)

# Predict on test set
y_pred = dt_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Optional: Visualize the Decision Tree
plt.figure(figsize=(20,10))
tree.plot_tree(dt_model, feature_names=X.columns, class_names=['Not Survived','Survived'], filled=True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Input features and target
X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
        'Embarked_C', 'Embarked_Q', 'Embarked_S',
        'Sex_female', 'Sex_male']]
y = df['Survived']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Random Forest model
rf_model = RandomForestClassifier(
    n_estimators=100,      # number of trees
    max_depth=7,           # maximum depth of each tree
    min_samples_split=2,   # minimum samples to split a node
    min_samples_leaf=1,    # minimum samples in a leaf node
    max_features='sqrt',   # number of features to consider at each split
    random_state=42
)

# Train the model
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Optional: Feature importance
import matplotlib.pyplot as plt
import seaborn as sns

feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances in Random Forest')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Input features and target
X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
        'Embarked_C', 'Embarked_Q', 'Embarked_S',
        'Sex_female', 'Sex_male']]
y = df['Survived']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Hyperparameters:", grid_search.best_params_)

# Predict with best model
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Feature importance
import matplotlib.pyplot as plt
import seaborn as sns

feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances (Best Random Forest)')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Input features and target
X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
        'Embarked_C', 'Embarked_Q', 'Embarked_S',
        'Sex_female', 'Sex_male']]
y = df['Survived']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

# GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Hyperparameters:", grid_search.best_params_)

# Predict with best model
best_xgb = grid_search.best_estimator_
y_pred = best_xgb.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Feature importance
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_xgb.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances (Best XGBoost)')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Input features and target
X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
        'Embarked_C', 'Embarked_Q', 'Embarked_S',
        'Sex_female', 'Sex_male']]
y = df['Survived']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Base XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, random_state=42)

# Hyperparameter grid including different objective and booster (optimization) functions
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'objective': ['binary:logistic', 'binary:hinge', 'count:poisson'],
    'booster': ['gbtree', 'gblinear', 'dart']
}

# GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           cv=3, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Hyperparameters:", grid_search.best_params_)

# Predict with best model
best_xgb = grid_search.best_estimator_
y_pred = best_xgb.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Feature importance
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_xgb.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances (Best XGBoost)')
plt.show()



import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Input features and target
X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
        'Embarked_C', 'Embarked_Q', 'Embarked_S',
        'Sex_female', 'Sex_male']]
y = df['Survived']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

# GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Hyperparameters:", grid_search.best_params_)

# Predict with best model
best_xgb = grid_search.best_estimator_
y_pred = best_xgb.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Feature importance
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_xgb.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances (Best XGBoost)')
plt.show()